{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from band.model import TFBertForSequenceClassification\n",
    "from band.dataset import ChnSentiCorp\n",
    "from band.progress import classification_convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 1\n",
    "MAX_SEQ_LEN = 128\n",
    "LEARNING_RATE = 3e-5\n",
    "SAVE_MODEL = False\n",
    "pretrained_dir = \"/home/band/models\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Dataset /tmp/band\\datasets\\chnsenticorp already cached.",
      "\n",
      "+-------------+-------------+-------------+--------------+---------------+-----------------+\n| Information | Data Number | Text MaxLen | Text Min_len | Text Mean_len | Recommended Len |\n+-------------+-------------+-------------+--------------+---------------+-----------------+\n|    train    |     9600    |     1992    |      4       |      108      |       315       |\n|  validation |     1200    |     924     |      15      |      107      |       295       |\n|     test    |     1200    |     1992    |      18      |      106      |       322       |\n| Text A Info |             |             |              |               |                 |\n+-------------+-------------+-------------+--------------+---------------+-----------------+",
      "\n",
      "+-------------+------+------+\n| Information |  1   |  0   |\n+-------------+------+------+\n|    train    | 4799 | 4801 |\n|  validation | 593  | 607  |\n|     test    | 608  | 592  |\n|  Label Info |      |      |\n+-------------+------+------+",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "dataset = ChnSentiCorp(save_path=\"/tmp/band\")\n",
    "data, label = dataset.data, dataset.label\n",
    "dataset.dataset_information()\n",
    "train_number, eval_number, test_number = dataset.train_examples_num, dataset.eval_examples_num, dataset.test_examples_num"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a48b995c327b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m train_dataset = classification_convert_examples_to_features(data['train'], tokenizer, max_length=MAX_SEQ_LEN,\n\u001b[0;32m      3\u001b[0m                                                             \u001b[0mlabel_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                                             output_mode=\"classification\")\n\u001b[0;32m      5\u001b[0m valid_dataset = classification_convert_examples_to_features(data['validation'], tokenizer, max_length=MAX_SEQ_LEN,\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \"\"\"\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms3_models\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m                         list(cls.vocab_files_names.values())))\n\u001b[0m\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[1;31m# Get files from url, cache, or disk depending on the case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Model name '/home/band/models' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). We assumed '/home/band/models' was a path or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
     ],
     "ename": "OSError",
     "evalue": "Model name '/home/band/models' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). We assumed '/home/band/models' was a path or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.",
     "output_type": "error"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(pretrained_dir)\n",
    "train_dataset = classification_convert_examples_to_features(data['train'], tokenizer, max_length=MAX_SEQ_LEN,\n",
    "                                                            label_list=label,\n",
    "                                                            output_mode=\"classification\")\n",
    "valid_dataset = classification_convert_examples_to_features(data['validation'], tokenizer, max_length=MAX_SEQ_LEN,\n",
    "                                                            label_list=label,\n",
    "                                                            output_mode=\"classification\")\n",
    "test_dataset = classification_convert_examples_to_features(data['test'], tokenizer, max_length=MAX_SEQ_LEN,\n",
    "                                                           label_list=label,\n",
    "                                                           output_mode=\"classification\")\n",
    "\n",
    "train_dataset = train_dataset.shuffle(100).batch(BATCH_SIZE, drop_remainder=True).repeat(EPOCHS)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(EVAL_BATCH_SIZE)\n",
    "valid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(TEST_BATCH_SIZE)\n",
    "test_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(pretrained_dir, num_labels=dataset.num_labels)\n",
    "model = TFBertForSequenceClassification.from_pretrained(pretrained_dir, config=config, from_pt=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, epsilon=1e-08)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=EPOCHS,\n",
    "                    steps_per_epoch=train_number // BATCH_SIZE,\n",
    "                    validation_data=valid_dataset,\n",
    "                    validation_steps=eval_number // EVAL_BATCH_SIZE)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_dataset, steps=test_number // TEST_BATCH_SIZE)\n",
    "print(loss, accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if SAVE_MODEL:\n",
    "    saved_model_path = \"./saved_models/{}\".format(int(time.time()))\n",
    "    model.save(saved_model_path, save_format=\"tf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}